{
  "hash": "033fccdc92b9356165534b3cae7b1197",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"TensorFlow Developer Certification (1/5): Dasar-Dasar TensorFlow\"\nauthor: \"Akmal Fadhlurrahman\"\ndate: \"2024-04-08\"\ncategories: [data-science, tensorflow-certification]\n\ntoc: true\npage-layout: full\ntitle-block-banner-color : white\n---\n\n![](images/clipboard-2076072366.png)\n\n# Belajar karena *Voucher* Gratisan\n\nKenapa tiba-tiba muncul ide untuk buat blogpost tentang TensorFlow?\n\nJawabannya: sesungguhnya saya mendapatkan *voucher* [sertifikasi TensorFlow Developer](#0) gratis dari [Bangkit Academy](#0) (bernilai US\\$100) karena pernah menjadi kontributor di sana untuk beberapa sesi.\n\n![](images/clipboard-4177578384.png){width=\"525\"}\n\nDan oleh karena itu saya perlu mempelajari seluk-beluk pemrograman TensorFlow sebelum *voucher* tersebut kadaluarsa pada akhir Mei 2024 (tinggal 2 bulan lagi ketika blog ini ditulis...).\n\n![](images/clipboard-89538439.png){width=\"342\"}\n\nDan sejujurnya selama karir saya sebagai data scientist, saya belum mendalami teknik-teknik *deep learning* dengan seksama, apalagi dengan menggunakan *library* TensorFlow. (Saya belajar dengan *library* scikit-learn dan yang mirip-mirip dengan scikit-learn).\n\nJadi saya kemudian membeli tiga *course* dariUdemy: [TensorFlow Developer Certificate Bootcamp](https://www.udemy.com/course/tensorflow-developer-certificate-machine-learning-zero-to-mastery), [TensorFlow 2.0: Deep Learning and Artificial Intelligence](https://www.udemy.com/course/deep-learning-tensorflow-2), dan [A Deep Understanding of Deep Learning](https://www.udemy.com/course/deeplearning_x) lalu mulai belajar dari dasar-dasarnya. (Semoga dapat reimburse dari kantor, amin!)\n\nTapi kemudian course-course itu gak ditonton dan/atau diterapkan selama 2 bulan :) Padahal waktu belajarnya cuman 4 bulan :))\n\nJadi supaya ada *progress*, akhirnya dicobalah untuk *learn in public*: secara periodik nulis \"laporan\" yang dishare di publik, supaya jadi motivasi belajar.\n\n## Rencana #LearnInPublic\n\nBlog ini akan dipecah menjadi 5 bagian sesuai dengan [silabus ujian Tensorflow Developer](https://www.tensorflow.org/extras/cert/TF_Certificate_Candidate_Handbook.pdf)\n\n-   *TensorFlow developer skills* (**blog ini**)\n\n-   Membuat model *neural network* dengan TensorFlow 2.x\n\n-   Klasifikasi gambar (*image classification*)\n\n-   Pemrosesan bahasa alami (*natural language processing/NLP*)\n\n-   Model runtun waktu, barisan, dan peramalan (*time series, sequences, and predictions*) *â€“\\> ini emang terjemahannya bener ya??*\n\n# Instalasi TensorFlow\n\nDi Python, library TensorFlow (TF) **terbaru** bisa diinstall dari pip:\n\n``` py\npip install tensorflow\n```\n\nUntuk pengguna MacOS, TensorFlow versi 2.12 **ke bawah** bisa diinstall dengan\n\n``` {.python .py}\npip install tensorflow-macos\n```\n\nDan untuk MacOS dengan prosesor Apple Silicon, [direkomendasikan](https://developer.apple.com/metal/tensorflow-plugin/) untuk menginstall plugin `tensorflow-metal`:\n\n``` python\npip install tensorflow-metal\n```\n\nUntuk mengecek versi TF yang terinstall, gunakan `__version__`:\n\n::: {#323a953a .cell execution_count=1}\n``` {.python .cell-code}\nimport tensorflow as tf\nprint(tf.__version__)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.15.0\n```\n:::\n:::\n\n\n# TensorFlow dan GPU/TPU\n\nMengapa untuk MacOS dengan prosesor Apple Silicon kita perlu menginstall plugin `tensorflow-metal`? Karena keberadaan *hardware* GPU pada Apple Silicon dapat digunakan untuk mempercepat proses *training* model TensorFlow. Hal yang sama berlaku ketika menggunakan TensorFlow dengan Google Colab; penggunaan GPU/TPU akan mempercepat waktu *training* model-model TF.\n\nUntuk melihat *hardware* apa yang dapat digunakan oleh TF:\n\n::: {#962a31b9 .cell execution_count=2}\n``` {.python .cell-code}\ntf.config.list_physical_devices()\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n```\n:::\n:::\n\n\nTerlihat bahwa di sini kita bisa menggunakan GPU, karena saya membuat website ini di laptop dengan Apple Silicon.\n\n# Pengenalan Tensor\n\nStruktur data yang paling dasar dalam TensorFlow adalah Tensor (d'uh). Mengutip dari [dokumentasi resmi TensorFlow:](https://www.tensorflow.org/api_docs/python/tf)\n\n> *Tensors are multi-dimensional arrays with a uniform type (called a `dtype`). You can see all supported `dtypes` at [`tf.dtypes`](https://www.tensorflow.org/api_docs/python/tf/dtypes).*\n>\n> *If you're familiar with [NumPy](https://numpy.org/devdocs/user/quickstart.html), tensors are (kind of) like `np.arrays`.*\n>\n> *All tensors are immutable like Python numbers and strings: you can never update the contents of a tensor, only create a new one.*\n\nKarena Tensor pada dasarnya adalah array multidimensi, maka Tensor bisa berbentuk skalar (array berdimensi 0), vektor (array dimensi 1), matriks (array dimensi 2), tumpukan matriks (array dimensi 3), atau tumpukan dari tumpukan matriks (array dimensi 4), dan seterusnya\n\n![](images/clipboard-3871733839.png)![](images/clipboard-2123506374.png)![](images/clipboard-1706963052.png)![](images/clipboard-500934595.png)\n\n[**Contoh**]{.underline}: Ketika melakukan *image classification*, gambar atau video bisa diubah menjadi array dimensi 3 yang memisahkan *RGB color channel*-nya (red, green, blue).\n\n## Membuat Tensor\n\nUntuk membuat tensor, kita bisa menggunakan `tf.constant()`\n\n::: {#0f5aacd7 .cell execution_count=3}\n``` {.python .cell-code}\na = tf.constant(1)\nb = tf.constant([1, 2])\nc = tf.constant([\n  [1,2],\n  [3,4]\n])\nd = tf.constant([\n  [\n    [1,2],\n    [3,4]\n  ],\n  [\n    [5,6],\n    [7,8]\n  ]\n])\n```\n:::\n\n\n::: {#88faf301 .cell execution_count=4}\n``` {.python .cell-code}\nprint(a) # Skalar\nprint(b) # Vektor\nprint(c) # Matriks\nprint(d) # Tumpukan matriks\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor([1 2], shape=(2,), dtype=int32)\ntf.Tensor(\n[[1 2]\n [3 4]], shape=(2, 2), dtype=int32)\ntf.Tensor(\n[[[1 2]\n  [3 4]]\n\n [[5 6]\n  [7 8]]], shape=(2, 2, 2), dtype=int32)\n```\n:::\n:::\n\n\nKadangkala kita perlu membuat tensor dengan isi nol semua, kita bisa menggunakan `tf.zeros`:\n\n::: {#cec6577c .cell execution_count=5}\n``` {.python .cell-code}\nrank4 = tf.zeros(shape=[2,3,4,5])\nrank4\n```\n\n::: {.cell-output .cell-output-display execution_count=45}\n```\n<tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=\narray([[[[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]]],\n\n\n       [[[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]]]], dtype=float32)>\n```\n:::\n:::\n\n\natau untuk membangkitkan tensor acak, gunakan `tf.random.Generator`\n\n::: {#d6ffc6f6 .cell execution_count=6}\n``` {.python .cell-code}\nrandom1 = tf.random.Generator.from_seed(39209)\nrandom1 = random1.normal(shape=(3,2))\nprint(random1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 0.43202025  0.88672817]\n [-1.2629851   0.4553509 ]\n [ 0.6588549   1.129419  ]], shape=(3, 2), dtype=float32)\n```\n:::\n:::\n\n\n## Membuat Variable\n\nSelain `Tensor`, terdapat jenis data lain yang ada di TF, yakni `Variable`. Sesuai namanya, `Variable` adalah Tensor yang nilainya dapat diubah-ubah dengan menggunakan metode `.assign()`\n\n::: {#efd10814 .cell execution_count=7}\n``` {.python .cell-code}\nchangeable_tensor = tf.Variable([10, 7])\nprint(changeable_tensor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'Variable:0' shape=(2,) dtype=int32, numpy=array([10,  7], dtype=int32)>\n```\n:::\n:::\n\n\n::: {#8f63facd .cell execution_count=8}\n``` {.python .cell-code}\nchangeable_tensor.assign([3, 5])\n```\n\n::: {.cell-output .cell-output-display execution_count=48}\n```\n<tf.Variable 'UnreadVariable' shape=(2,) dtype=int32, numpy=array([3, 5], dtype=int32)>\n```\n:::\n:::\n\n\n`Variable` digunakan oleh TensorFlow salah satunya untuk melakukan diferensiasi (menghitung $\\dfrac{\\partial f}{\\partial w_i}$) dan menyimpan nilai bobot ($w_i$) ketika *training* model.\n\nNamun apabila kita memiliki Variable yang tidak perlu di-*train* (misal: step counter), maka kita bisa menyetel variable sebagai *non-trainable*, contoh:\n\n::: {#7c091ae3 .cell execution_count=9}\n``` {.python .cell-code}\nstep_counter = tf.Variable(1, trainable=False)\n```\n:::\n\n\n## Melihat Informasi Tensor\n\nSeperti array Numpy, kita bisa menarik beberapa informasi seputar tensor:\n\n::: {#4356017e .cell execution_count=10}\n``` {.python .cell-code}\nprint(rank4.dtype) # Tipe data dtype di tensor rank4\nprint(rank4.shape) # Bentuk data tensor\nprint(rank4.ndim)  # Jumlah dimensi tensor (karena datanya berbentuk 2x3x4x5 --> 4 dimensi)\nprint(tf.size(rank4).numpy()) # Karena bentuk datanya 2x3x4x5 maka terdapat 120 item dalam tensor rank4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<dtype: 'float32'>\n(2, 3, 4, 5)\n4\n120\n```\n:::\n:::\n\n\n## Manipulasi Tensor\n\nKita juga menggunakan *indexing* untuk melihat sebagian dari tensor, misal:\n\n::: {#2c951a11 .cell execution_count=11}\n``` {.python .cell-code}\n# Get the first 2 elements of each dimension\nrank4[:1, :3, :2, :2]\n```\n\n::: {.cell-output .cell-output-display execution_count=51}\n```\n<tf.Tensor: shape=(1, 3, 2, 2), dtype=float32, numpy=\narray([[[[0., 0.],\n         [0., 0.]],\n\n        [[0., 0.],\n         [0., 0.]],\n\n        [[0., 0.],\n         [0., 0.]]]], dtype=float32)>\n```\n:::\n:::\n\n\nDan apabila diperlukan, kita bisa menambahkan dimensi baru dari tensor *existing* dengan menggunakan `tf.newaxis`:\n\n::: {#4b40c60c .cell execution_count=12}\n``` {.python .cell-code}\n# Crate a r2x2 tensor (2 dimensions)\nrank2 = tf.constant([[10, 3],\n                     [2, 5]])\n                     \n# Add in extra dimension to our rank 2 tensor toi make 2x2x1 tensor\nrank3_exp = rank2[..., tf.newaxis]\nrank3_exp\n```\n\n::: {.cell-output .cell-output-display execution_count=52}\n```\n<tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=\narray([[[10],\n        [ 3]],\n\n       [[ 2],\n        [ 5]]], dtype=int32)>\n```\n:::\n:::\n\n\nApabila diperlukan, kita juga bisa melakukan kebalikannya: Menghilangkan semua *single dimensions* dengan fungsi `tf.squeeze():`\n\n::: {#50b91a5d .cell execution_count=13}\n``` {.python .cell-code}\n# Create a tensor to get started\ntf.random.set_seed(42)\nG = tf.constant(tf.random.uniform(shape=[50]), shape=(1,1,1,2,25))\nG\n```\n\n::: {.cell-output .cell-output-display execution_count=53}\n```\n<tf.Tensor: shape=(1, 1, 1, 2, 25), dtype=float32, numpy=\narray([[[[[0.6645621 , 0.44100678, 0.3528825 , 0.46448255, 0.03366041,\n           0.68467236, 0.74011743, 0.8724445 , 0.22632635, 0.22319686,\n           0.3103881 , 0.7223358 , 0.13318717, 0.5480639 , 0.5746088 ,\n           0.8996835 , 0.00946367, 0.5212307 , 0.6345445 , 0.1993283 ,\n           0.72942245, 0.54583454, 0.10756552, 0.6767061 , 0.6602763 ],\n          [0.33695042, 0.60141766, 0.21062577, 0.8527372 , 0.44062173,\n           0.9485276 , 0.23752594, 0.81179297, 0.5263394 , 0.494308  ,\n           0.21612847, 0.8457197 , 0.8718841 , 0.3083862 , 0.6868038 ,\n           0.23764038, 0.7817228 , 0.9671384 , 0.06870162, 0.79873943,\n           0.66028714, 0.5871513 , 0.16461694, 0.7381023 , 0.32054043]]]]],\n      dtype=float32)>\n```\n:::\n:::\n\n\n::: {#047fd5b1 .cell execution_count=14}\n``` {.python .cell-code}\nG_squeezed = tf.squeeze(G)\nG_squeezed, G_squeezed.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=54}\n```\n(<tf.Tensor: shape=(2, 25), dtype=float32, numpy=\n array([[0.6645621 , 0.44100678, 0.3528825 , 0.46448255, 0.03366041,\n         0.68467236, 0.74011743, 0.8724445 , 0.22632635, 0.22319686,\n         0.3103881 , 0.7223358 , 0.13318717, 0.5480639 , 0.5746088 ,\n         0.8996835 , 0.00946367, 0.5212307 , 0.6345445 , 0.1993283 ,\n         0.72942245, 0.54583454, 0.10756552, 0.6767061 , 0.6602763 ],\n        [0.33695042, 0.60141766, 0.21062577, 0.8527372 , 0.44062173,\n         0.9485276 , 0.23752594, 0.81179297, 0.5263394 , 0.494308  ,\n         0.21612847, 0.8457197 , 0.8718841 , 0.3083862 , 0.6868038 ,\n         0.23764038, 0.7817228 , 0.9671384 , 0.06870162, 0.79873943,\n         0.66028714, 0.5871513 , 0.16461694, 0.7381023 , 0.32054043]],\n       dtype=float32)>,\n TensorShape([2, 25]))\n```\n:::\n:::\n\n\n## Mengubah *data type* tensor\n\nSejauh ini, kita hanya bekerja dengan tensor ber-`dtype` `int32`. Mari kita membuat sebuah tensor bertipe `float`:\n\n::: {#790f1eea .cell execution_count=15}\n``` {.python .cell-code}\nt1 = tf.constant([[1,2], [3,4]], dtype=float)\nt2 = tf.constant([[5,6], [7,8]], dtype=float)\nt1, t2\n```\n\n::: {.cell-output .cell-output-display execution_count=55}\n```\n(<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n array([[1., 2.],\n        [3., 4.]], dtype=float32)>,\n <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n array([[5., 6.],\n        [7., 8.]], dtype=float32)>)\n```\n:::\n:::\n\n\nUntuk beberapa kasus, kita perlu mengubah tensor yang sudah ada menjadi data type yang lain. Untuk itu, kita bisa menggunakan fungsi `tf.cast()`\n\n::: {#4f3b3468 .cell execution_count=16}\n``` {.python .cell-code}\nimport numpy as np\n# Create a random tensor of values between -100 and 100 of size 50\nE = tf.constant(np.random.randint(-100,100,size = 50))\nE\n```\n\n::: {.cell-output .cell-output-display execution_count=56}\n```\n<tf.Tensor: shape=(50,), dtype=int64, numpy=\narray([ 17,  10, -69, -39,  59, -76, -63,  65,  62,  12, -78, -80, -79,\n        14,  81,  54, -78,  70, -22,  13, -61, -59, -66, -89, -21, -66,\n       -98, -44,  64,  96,  96, -12, -28, -19, -56, -99, -77,  86, -94,\n        23,  90,  -2,  61, -96,  61, -18, -74, -17,  93,  42])>\n```\n:::\n:::\n\n\n::: {#8a38e20b .cell execution_count=17}\n``` {.python .cell-code}\ntf.cast(E, dtype=tf.float32)\n```\n\n::: {.cell-output .cell-output-display execution_count=57}\n```\n<tf.Tensor: shape=(50,), dtype=float32, numpy=\narray([ 17.,  10., -69., -39.,  59., -76., -63.,  65.,  62.,  12., -78.,\n       -80., -79.,  14.,  81.,  54., -78.,  70., -22.,  13., -61., -59.,\n       -66., -89., -21., -66., -98., -44.,  64.,  96.,  96., -12., -28.,\n       -19., -56., -99., -77.,  86., -94.,  23.,  90.,  -2.,  61., -96.,\n        61., -18., -74., -17.,  93.,  42.], dtype=float32)>\n```\n:::\n:::\n\n\n## Operasi Tensor\n\nUntuk menjumlahkan, mengalikan dengan skalar, dan perkalian matriks, operator Python standar `+`, `*`, `@` tetap bisa digunakan:\n\n::: {#7f44065d .cell execution_count=18}\n``` {.python .cell-code}\nprint(t1 + t2, '\\n')\nprint(t1 * 1/3, '\\n')\nprint(t1 @ t2, '\\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 6.  8.]\n [10. 12.]], shape=(2, 2), dtype=float32) \n\ntf.Tensor(\n[[0.33333334 0.6666667 ]\n [1.         1.3333334 ]], shape=(2, 2), dtype=float32) \n\ntf.Tensor(\n[[19. 22.]\n [43. 50.]], shape=(2, 2), dtype=float32) \n\n```\n:::\n:::\n\n\nMeskipun demikian, *best practice* untuk melakukan operasi-operasi tersebut adalah dengan menggunakan fungsi-fungsi *native* Tensorflow, seperti `tf.add`, `tf.multiply`, dan `tf.matmul:`\n\n::: {#bb55f1a8 .cell execution_count=19}\n``` {.python .cell-code}\nprint(tf.add(t1, t2), \"\\n\")\nprint(tf.multiply(t1, 1/3), \"\\n\")\nprint(tf.matmul(t1, t2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 6.  8.]\n [10. 12.]], shape=(2, 2), dtype=float32) \n\ntf.Tensor(\n[[0.33333334 0.6666667 ]\n [1.         1.3333334 ]], shape=(2, 2), dtype=float32) \n\ntf.Tensor(\n[[19. 22.]\n [43. 50.]], shape=(2, 2), dtype=float32) \n\n```\n:::\n:::\n\n\n## Agregasi Tensor\n\nUntuk mencari nilai min/max/mean/sum/variance dari suatu tensor, kita bisa menggunakan fungsi-fungsi reduksi berikut:\n\n::: {#f8bf253a .cell execution_count=20}\n``` {.python .cell-code}\n# Create a random tensor of values between -100 and 100 of size 50\nE = tf.constant(np.random.randint(-100,100,size = 50))\nprint(E)\n\n# Find the minimum\nprint(\"Minimum: \", tf.reduce_min(E).numpy())\n\n# Find the maximum\nprint(\"Maximum: \", tf.reduce_max(E).numpy())\n\n# Find the average\nprint(\"Average: \", tf.reduce_mean(E).numpy())\n\n# Find the sum\nprint(\"Total: \", tf.reduce_sum(E).numpy())\n\n# Find the stdev and variance\n# Notice that we convert the tensor into float dtype here\nprint(\"Standard deviation: \", tf.math.reduce_std(tf.cast(E, dtype=tf.float32)).numpy())\nprint(\"Variance: \", tf.math.reduce_variance(tf.cast(E, dtype=tf.float32)).numpy())\n\n# Where the positional minimum and maximum of a tensor (argmin and argmax)\nprint(\"Positional minimum: \", tf.argmin(E).numpy())\nprint(\"Positional maximum: \", tf.argmax(E).numpy())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[ 53  58 -96  79  55  27  -8   4  11  86 -96  73 -78 -42   5 -10 -53 -81\n -19  -5 -53 -46  96 -57  23 -61  89 -72  35 -55 -31  -6 -19  73  35  62\n  91  91  49 -54 -28 -48 -96 -78  35 -53   3  45 -82 -48], shape=(50,), dtype=int64)\nMinimum:  -96\nMaximum:  96\nAverage:  -3\nTotal:  -197\nStandard deviation:  58.411095\nVariance:  3411.8562\nPositional minimum:  2\nPositional maximum:  22\n```\n:::\n:::\n\n\n# \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}